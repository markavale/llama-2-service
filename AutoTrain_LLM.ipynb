{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "JvMRbVLEJlZT"
   },
   "outputs": [],
   "source": [
    "#@title ü§ó AutoTrain LLM\n",
    "#@markdown In order to use this colab\n",
    "#@markdown - upload train.csv to a folder named `data/`\n",
    "#@markdown - train.csv must contain a `text` column\n",
    "#@markdown - choose a project name if you wish\n",
    "#@markdown - change model if you wish, you can use most of the text-generation models from Hugging Face Hub\n",
    "#@markdown - add huggingface information (token and repo_id) if you wish to push trained model to huggingface hub\n",
    "#@markdown - update hyperparameters if you wish\n",
    "#@markdown - click `Runtime > Run all` or run each cell individually\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/mnt/repo/text-generation-webui/models/meta-llama_Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "A2-_lkBS1WKA"
   },
   "outputs": [],
   "source": [
    "#@markdown ---\n",
    "#@markdown #### Project Config\n",
    "#@markdown Note: if you are using a restricted/private model, you need to enter your Hugging Face token in the next step.\n",
    "project_name = 'my_autotrain_llm' # @param {type:\"string\"}\n",
    "model_name = MODEL_PATH # @param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Push to Hub?\n",
    "#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
    "#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
    "#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
    "#@markdown You can find your token here: https://huggingface.co/settings/tokens\n",
    "push_to_hub = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "hf_token = \"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\" #@param {type:\"string\"}\n",
    "repo_id = \"markavale/mmi-llama2-7b-chat\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Hyperparameters\n",
    "learning_rate = 2e-4 # @param {type:\"number\"}\n",
    "num_epochs = 1 #@param {type:\"number\"}\n",
    "batch_size = 1 # @param {type:\"slider\", min:1, max:32, step:1}\n",
    "block_size = 1024 # @param {type:\"number\"}\n",
    "trainer = \"sft\" # @param [\"default\", \"sft\"] {type:\"raw\"}\n",
    "warmup_ratio = 0.1 # @param {type:\"number\"}\n",
    "weight_decay = 0.01 # @param {type:\"number\"}\n",
    "gradient_accumulation = 4 # @param {type:\"number\"}\n",
    "use_fp16 = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "use_peft = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "use_int4 = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "lora_r = 16 #@param {type:\"number\"}\n",
    "lora_alpha = 32 #@param {type:\"number\"}\n",
    "lora_dropout = 0.05 #@param {type:\"number\"}\n",
    "\n",
    "os.environ[\"PROJECT_NAME\"] = project_name\n",
    "os.environ[\"MODEL_NAME\"] = model_name\n",
    "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "os.environ[\"REPO_ID\"] = repo_id\n",
    "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
    "os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n",
    "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
    "os.environ[\"BLOCK_SIZE\"] = str(block_size)\n",
    "os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n",
    "os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n",
    "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
    "os.environ[\"USE_FP16\"] = str(use_fp16)\n",
    "os.environ[\"USE_PEFT\"] = str(use_peft)\n",
    "os.environ[\"USE_INT4\"] = str(use_int4)\n",
    "os.environ[\"LORA_R\"] = str(lora_r)\n",
    "os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n",
    "os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3cd_ED_yXXt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1mINFO    Running LLM\u001b[0m\n",
      "> \u001b[1mINFO    Params: Namespace(add_eos_token=False, auto_find_batch_size=False, backend='default', block_size=1024, data_path='data/', deploy=False, evaluation_strategy='epoch', fp16=True, func=<function run_llm_command_factory at 0x7f15dbb2fe50>, gradient_accumulation_steps=4, inference=False, learning_rate=0.0002, logging_steps=-1, lora_alpha=32, lora_dropout=0.05, lora_r=16, max_grad_norm=1.0, merge_adapter=False, model='/mnt/repo/text-generation-webui/models/meta-llama_Llama-2-7b-chat-hf', model_max_length=1024, num_train_epochs=1, optimizer='adamw_torch', project_name='my_autotrain_llm', push_to_hub=True, repo_id='markavale/mmi-llama2-7b-chat', save_strategy='epoch', save_total_limit=1, scheduler='linear', seed=42, target_modules=None, text_column='text', token='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', train=True, train_batch_size=1, train_split='train', trainer='default', use_int4=True, use_int8=False, use_peft=True, username=None, valid_split=None, version=False, warmup_ratio=0.1, weight_decay=0.01)\u001b[0m\n",
      "> \u001b[1mINFO    loading dataset from csv\u001b[0m\n",
      "/mnt/repo/Llama2RAG/venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:631: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "/mnt/repo/Llama2RAG/venv/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:995: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/mnt/repo/Llama2RAG/venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:460: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.34s/it]\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 32000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "Running tokenizer on train dataset:   0%|       | 0/9846 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1109 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Running tokenizer on train dataset: 100%|‚ñà| 9846/9846 [00:01<00:00, 8543.16 exam\n",
      "Grouping texts in chunks of 1024 (num_proc=4): 100%|‚ñà| 9846/9846 [00:01<00:00, 9\n",
      "> \u001b[1mINFO    creating trainer\u001b[0m\n",
      "{'loss': 1.3055, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.8}         \n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 894/1081 [1:21:48<17:11,  5.52s/it]"
     ]
    }
   ],
   "source": [
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model ${MODEL_NAME} \\ app.py                Home.py                  my_autotrain_llm        server.\n",
    "--project-name ${PROJECT_NAME} \\\n",
    "--data-path data/ \\\n",
    "--text-column text \\\n",
    "--lr ${LEARNING_RATE} \\\n",
    "--batch-size ${BATCH_SIZE} \\\n",
    "--epochs ${NUM_EPOCHS} \\\n",
    "--block-size ${BLOCK_SIZE} \\\n",
    "--warmup-ratio ${WARMUP_RATIO} \\\n",
    "--lora-r ${LORA_R} \\\n",
    "--lora-alpha ${LORA_ALPHA} \\\n",
    "--lora-dropout ${LORA_DROPOUT} \\\n",
    "--weight-decay ${WEIGHT_DECAY} \\\n",
    "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
    "$( [[ \"$USE_FP16\" == \"True\" ]] && echo \"--fp16\" ) \\\n",
    "$( [[ \"$USE_PEFT\" == \"True\" ]] && echo \"--use-peft\" ) \\\n",
    "$( [[ \"$USE_INT4\" == \"True\" ]] && echo \"--use-int4\" ) \\\n",
    "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
